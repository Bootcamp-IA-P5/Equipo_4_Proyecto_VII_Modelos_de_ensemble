{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee0e44f",
   "metadata": {},
   "source": [
    "# Modelos de Ensemble\n",
    "## Proyecto de Clasificación Multiclase\n",
    "\n",
    "Este notebook implementa modelos de ensemble avanzados:\n",
    "- **Bagging**: BaggingClassifier\n",
    "- **Boosting**: AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
    "- **Stacking**: Combinación de múltiples modelos\n",
    "- **Voting**: Soft y Hard Voting\n",
    "\n",
    "Los modelos de ensemble combinan múltiples modelos para mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7290edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Modelos Ensemble\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelos base\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db4118",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "X_train = np.load('../data/processed/X_train_selected.npy')\n",
    "X_test = np.load('../data/processed/X_test_selected.npy')\n",
    "y_train = np.load('../data/processed/y_train_resampled.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "# Cargar label encoder\n",
    "with open('../models/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Número de clases: {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6687db1",
   "metadata": {},
   "source": [
    "## 2. Función de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda52435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo ensemble\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluando: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    precision = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n📊 Resultados:\")\n",
    "    print(f\"  Tiempo de entrenamiento: {training_time:.2f}s\")\n",
    "    print(f\"  Accuracy (Train): {train_acc:.4f}\")\n",
    "    print(f\"  Accuracy (Test): {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test, \n",
    "                                target_names=label_encoder.classes_,\n",
    "                                zero_division=0))\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'train_accuracy': float(train_acc),\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'training_time': float(training_time)\n",
    "    }\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    return model, results, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eada23",
   "metadata": {},
   "source": [
    "## 3. Modelos de Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a67d3d",
   "metadata": {},
   "source": [
    "### 3.1 BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bagging_model, bagging_results, bagging_cm = evaluate_ensemble(\n",
    "    bagging_model, X_train, X_test, y_train, y_test, \"Bagging Classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9459560",
   "metadata": {},
   "source": [
    "### 3.2 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f53b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_model = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et_model, et_results, et_cm = evaluate_ensemble(\n",
    "    et_model, X_train, X_test, y_train, y_test, \"Extra Trees\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78fc30",
   "metadata": {},
   "source": [
    "## 4. Modelos de Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813080a",
   "metadata": {},
   "source": [
    "### 4.1 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_model, ada_results, ada_cm = evaluate_ensemble(\n",
    "    ada_model, X_train, X_test, y_train, y_test, \"AdaBoost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820a897",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e373d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model, gb_results, gb_cm = evaluate_ensemble(\n",
    "    gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990cdc3",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "xgb_model, xgb_results, xgb_cm = evaluate_ensemble(\n",
    "    xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f55f59",
   "metadata": {},
   "source": [
    "### 4.4 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_model, lgbm_results, lgbm_cm = evaluate_ensemble(\n",
    "    lgbm_model, X_train, X_test, y_train, y_test, \"LightGBM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae3265",
   "metadata": {},
   "source": [
    "## 5. Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelos base previamente entrenados\n",
    "with open('../models/logistic_regression_model.pkl', 'rb') as f:\n",
    "    lr_base = pickle.load(f)\n",
    "with open('../models/random_forest_model.pkl', 'rb') as f:\n",
    "    rf_base = pickle.load(f)\n",
    "\n",
    "# Voting Classifier (Soft Voting)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=50, random_state=42, eval_metric='mlogloss'))\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_model, voting_results, voting_cm = evaluate_ensemble(\n",
    "    voting_model, X_train, X_test, y_train, y_test, \"Voting Classifier (Soft)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f971fdd0",
   "metadata": {},
   "source": [
    "## 6. Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=50, random_state=42, eval_metric='mlogloss'))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model, stacking_results, stacking_cm = evaluate_ensemble(\n",
    "    stacking_model, X_train, X_test, y_train, y_test, \"Stacking Classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0d630",
   "metadata": {},
   "source": [
    "## 7. Comparación de Modelos Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36888c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar resultados\n",
    "ensemble_results = [\n",
    "    bagging_results,\n",
    "    et_results,\n",
    "    ada_results,\n",
    "    gb_results,\n",
    "    xgb_results,\n",
    "    lgbm_results,\n",
    "    voting_results,\n",
    "    stacking_results\n",
    "]\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_results)\n",
    "ensemble_df = ensemble_df.sort_values('test_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARACIÓN DE MODELOS ENSEMBLE\")\n",
    "print(\"=\"*100)\n",
    "print(ensemble_df.to_string(index=False))\n",
    "\n",
    "# Guardar\n",
    "ensemble_df.to_csv('../results/ensemble_models_comparison.csv', index=False)\n",
    "print(\"\\n✓ Resultados guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Test Accuracy\n",
    "ax1 = axes[0, 0]\n",
    "sorted_df = ensemble_df.sort_values('test_accuracy')\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_df)))\n",
    "ax1.barh(sorted_df['model_name'], sorted_df['test_accuracy'], color=colors, edgecolor='black')\n",
    "ax1.set_xlabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Test Accuracy - Modelos Ensemble', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim([0, 1])\n",
    "for i, v in enumerate(sorted_df['test_accuracy']):\n",
    "    ax1.text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# F1-Score\n",
    "ax2 = axes[0, 1]\n",
    "sorted_f1 = ensemble_df.sort_values('f1_score')\n",
    "ax2.barh(sorted_f1['model_name'], sorted_f1['f1_score'], color='coral', edgecolor='black')\n",
    "ax2.set_xlabel('F1-Score', fontsize=12)\n",
    "ax2.set_title('F1-Score - Modelos Ensemble', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "for i, v in enumerate(sorted_f1['f1_score']):\n",
    "    ax2.text(v + 0.01, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# Tiempo de entrenamiento\n",
    "ax3 = axes[1, 0]\n",
    "sorted_time = ensemble_df.sort_values('training_time')\n",
    "ax3.barh(sorted_time['model_name'], sorted_time['training_time'], color='lightgreen', edgecolor='black')\n",
    "ax3.set_xlabel('Tiempo (segundos)', fontsize=12)\n",
    "ax3.set_title('Tiempo de Entrenamiento', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(sorted_time['training_time']):\n",
    "    ax3.text(v + 0.5, i, f'{v:.1f}s', va='center', fontweight='bold')\n",
    "\n",
    "# Todas las métricas\n",
    "ax4 = axes[1, 1]\n",
    "metrics = ['test_accuracy', 'precision', 'recall', 'f1_score']\n",
    "x = np.arange(len(ensemble_df))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax4.bar(x + i*width, ensemble_df[metric], width, label=metric.replace('_', ' ').title())\n",
    "\n",
    "ax4.set_xlabel('Modelos', fontsize=12)\n",
    "ax4.set_ylabel('Score', fontsize=12)\n",
    "ax4.set_title('Todas las Métricas', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x + width * 1.5)\n",
    "ax4.set_xticklabels(ensemble_df['model_name'], rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/ensemble_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f609d",
   "metadata": {},
   "source": [
    "## 8. Mejor Modelo Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ensemble_name = ensemble_df.iloc[0]['model_name']\n",
    "best_ensemble_acc = ensemble_df.iloc[0]['test_accuracy']\n",
    "\n",
    "print(f\"\\n🏆 MEJOR MODELO ENSEMBLE: {best_ensemble_name}\")\n",
    "print(f\"   Test Accuracy: {best_ensemble_acc:.4f}\")\n",
    "print(f\"\\nTop 3 Modelos Ensemble:\")\n",
    "print(ensemble_df[['model_name', 'test_accuracy', 'f1_score', 'training_time']].head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86549b0f",
   "metadata": {},
   "source": [
    "## 9. Guardar Modelos Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544deadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelos\n",
    "ensemble_models = {\n",
    "    'bagging': bagging_model,\n",
    "    'extra_trees': et_model,\n",
    "    'adaboost': ada_model,\n",
    "    'gradient_boosting': gb_model,\n",
    "    'xgboost': xgb_model,\n",
    "    'lightgbm': lgbm_model,\n",
    "    'voting': voting_model,\n",
    "    'stacking': stacking_model\n",
    "}\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    with open(f'../models/{name}_ensemble.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "print(\"✓ Modelos ensemble guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09a130",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Se han implementado y evaluado 8 modelos de ensemble:\n",
    "- ✓ Bagging Classifier\n",
    "- ✓ Extra Trees\n",
    "- ✓ AdaBoost\n",
    "- ✓ Gradient Boosting\n",
    "- ✓ XGBoost\n",
    "- ✓ LightGBM\n",
    "- ✓ Voting Classifier\n",
    "- ✓ Stacking Classifier\n",
    "\n",
    "### Observaciones:\n",
    "1. Los modelos de boosting (XGBoost, LightGBM, Gradient Boosting) generalmente superan a bagging\n",
    "2. Stacking y Voting pueden combinar fortalezas de múltiples modelos\n",
    "3. Hay un trade-off entre precisión y tiempo de entrenamiento\n",
    "\n",
    "### Próximo paso:\n",
    "Optimización de hiperparámetros del mejor modelo"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
